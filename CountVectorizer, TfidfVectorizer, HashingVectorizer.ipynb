{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVectorizer, TfidfVectorizer, HashingVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text must be parsed to remove words, called tokenization. Then the words need to be encoded as integers \n",
    "or floating point values for use as input to a machine learning algorithm, called feature extraction \n",
    "(or vectorization).\n",
    "\n",
    "The scikit-learn library offers easy-to-use tools to perform both tokenization and feature extraction of \n",
    "your text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CountVectorizer provides a simple way to both tokenize a collection of text documents and build a \n",
    "vocabulary of known words, but also to encode new documents using that vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary is :\n",
      " {'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
      "The Shape of the vector text is :\n",
      " (1, 8)\n",
      "The type of the vector text is :\n",
      " <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Array of the vector text is :\n",
      " [[1 1 1 1 1 1 1 2]]\n"
     ]
    }
   ],
   "source": [
    "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "cv = CountVectorizer()\n",
    "cv.fit(text)\n",
    "vector_text = cv.transform(text)\n",
    "\n",
    "\n",
    "print('The vocabulary is :\\n', cv.vocabulary_)\n",
    "print('The Shape of the vector text is :\\n', vector_text.shape)\n",
    "print('The type of the vector text is :\\n', type(vector_text))\n",
    "print('Array of the vector text is :\\n',vector_text.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for every unique word CountVectorizer takes 1. Sometimes a specific word stay more than one time in a sentence \n",
    "in this situation CountVectorixer takes number based on the word number. Suppose, The word stay two time in a sence \n",
    "CountVectorizer will take 2 for two the. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One issue with simple counts is that some words like “the” will appear many times and their large counts will\n",
    "not be very meaningful in the encoded vectors.\n",
    "\n",
    "An alternative is to calculate word frequencies, and by far the most popular method is called TF-IDF. This is an acronym than stands for “Term Frequency – Inverse Document” Frequency which are the components of the \n",
    "resulting scores assigned to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary of tv is :\n",
      " {'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
      "Shape of vector_text is :\n",
      " (1, 8)\n",
      "Type of vector text is :\n",
      " <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Array of the vector text is :\n",
      " [[0.30151134 0.30151134 0.30151134 0.30151134 0.30151134 0.30151134\n",
      "  0.30151134 0.60302269]]\n"
     ]
    }
   ],
   "source": [
    "tv = TfidfVectorizer()\n",
    "tv.fit(text)\n",
    "vector_text = tv.transform(text)\n",
    "\n",
    "print('The vocabulary of tv is :\\n',tv.vocabulary_)\n",
    "print('Shape of vector_text is :\\n',vector_text.shape)\n",
    "print('Type of vector text is :\\n',type(vector_text))\n",
    "print('Array of the vector text is :\\n',vector_text.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TfidfVectorizer takes frequency based on the number of word on text. If a word stay two time in a text, this word\n",
    "will take more frequency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HashingVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counts and frequencies can be very useful, but one limitation of these methods is that the vocabulary can become very large.\n",
    "\n",
    "This, in turn, will require large vectors for encoding documents and impose large requirements on memory and slow down algorithms.\n",
    "\n",
    "A clever work around is to use a one way hash of words to convert them to integers. The clever part is that no vocabulary is required and you can choose an arbitrary-long fixed length vector. A downside is that the hash is a one-way function so there  is no way to convert the encoding back to a word (which may not matter for many supervised learning tasks).\n",
    "\n",
    "The HashingVectorizer class implements this approach that can be used to consistently hash words, then tokenize and encode documents as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the vector :\n",
      " (1, 20)\n",
      "Type of the vector_hv is : <class 'scipy.sparse.csr.csr_matrix'>\n",
      "array of the vector_hv is : [[ 0.          0.          0.          0.          0.          0.33333333\n",
      "   0.         -0.33333333  0.33333333  0.          0.          0.33333333\n",
      "   0.          0.          0.         -0.33333333  0.          0.\n",
      "  -0.66666667  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "hv = HashingVectorizer(n_features=20)\n",
    "hv.fit(text)\n",
    "vector_hv = hv.transform(text)\n",
    "\n",
    "print(\"shape of the vector :\\n\", vector_hv.shape)\n",
    "print('Type of the vector_hv is :',type(vector_hv))\n",
    "print('array of the vector_hv is :',vector_hv.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HashingVectorizer reduce the number of feature that is very helpful for algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
